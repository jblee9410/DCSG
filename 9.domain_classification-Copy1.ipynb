{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb64866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import easydict\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config\n",
    "from transformers import EvalPrediction\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import TrainingArguments, AdapterTrainer, EvalPrediction, AdapterSetup\n",
    "from transformers.adapters.composition import Fuse\n",
    "from transformers import GPT2AdapterModel\n",
    "from adapter_setting import TASK_ID_to_NAME, TASK_NAME_to_ID, TASK_DICT, DATA_ATTRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0ec4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper parameters\n",
    "args = easydict.EasyDict({\n",
    "    'model_dir_root' : './1.trained_model_single',\n",
    "    'seed' : 1234,\n",
    "    'adapter_type' : 'houlsby',\n",
    "    'stage1_epoch' : 5,\n",
    "    'tasks' : [0,1,2,3,4],\n",
    "    'stage1_batch_size' : 128,\n",
    "    'token_weight' : 5,\n",
    "    'data_dir' : './data',\n",
    "    'lm_lambda' : 0.25,\n",
    "    'lm_gen_percentage' : 0.2,\n",
    "    'max_len' : 128,\n",
    "    'debug' : False,\n",
    "    'verbose' : False\n",
    "})\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd18f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_dir(task_name):\n",
    "    return os.path.join(args.model_dir_root, task_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27c5db",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(task_name, dataset_type):\n",
    "    \n",
    "    train_dataset_path = TASK_DICT[task_name][dataset_type]\n",
    "    with open(train_dataset_path, 'r') as f:\n",
    "        raw_ds = json.load(f)\n",
    "        new_raw_ds = []\n",
    "        for i1 in range(len(raw_ds['data'])):\n",
    "            for i2 in range(len(raw_ds['data'][i1]['paragraphs'])):\n",
    "                raw_ds['data'][i1]['paragraphs'][i2]['pid'] = \"%d_%d\"%(i1, i2)\n",
    "            new_raw_ds.append(raw_ds[\"data\"][i1][\"paragraphs\"])\n",
    "        raw_ds = new_raw_ds\n",
    "\n",
    "    qa_input_list = []\n",
    "    for d in raw_ds:\n",
    "        context = d[0]['context']\n",
    "        question = d[0]['qas'][0]['question']\n",
    "        answer = d[0]['qas'][0]['answers'][0]['text']\n",
    "\n",
    "        qa_input = context + ' ' + question\n",
    "        if len(qa_input) > 128:\n",
    "            continue\n",
    "        qa_input_list.append(qa_input)\n",
    "        \n",
    "    return qa_input_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d0fdf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('extra_data.pickle', 'rb') as fr:\n",
    "    extra_dataset_dict = pickle.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8179004a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    train_dataset_dict = {'inputs':[],\n",
    "                          'labels':[]}\n",
    "    valid_dataset_dict = {'inputs':[],\n",
    "                          'labels':[]}\n",
    "    test_dataset_dict = {'inputs':[],\n",
    "                         'labels':[]}\n",
    "\n",
    "    for task_id in args.tasks:\n",
    "        task_name = TASK_ID_to_NAME[task_id]\n",
    "        train_qa_input_list = create_dataset(task_name, 'train')\n",
    "        valid_qa_input_list = create_dataset(task_name, 'eval')\n",
    "        test_qa_input_list = create_dataset(task_name, 'test')\n",
    "        \n",
    "        train_dataset_dict['inputs'].extend(train_qa_input_list)\n",
    "        train_dataset_dict['labels'].extend([task_id]*len(train_qa_input_list))\n",
    "        valid_dataset_dict['inputs'].extend(valid_qa_input_list)\n",
    "        valid_dataset_dict['labels'].extend([task_id]*len(valid_qa_input_list))\n",
    "        test_dataset_dict['inputs'].extend(test_qa_input_list)\n",
    "        test_dataset_dict['labels'].extend([task_id]*len(test_qa_input_list))\n",
    "        \n",
    "    return train_dataset_dict, valid_dataset_dict, test_dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5c8d2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"inputs\"], padding='max_length', truncation=True)\n",
    "\n",
    "train_dataset_dict, valid_dataset_dict, test_dataset_dict = get_dataset()\n",
    "\n",
    "dataset_dict = DatasetDict({'train':Dataset.from_dict(train_dataset_dict), \n",
    "                            'valid':Dataset.from_dict(valid_dataset_dict),\n",
    "                            'test':Dataset.from_dict(test_dataset_dict)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3770e758",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset_dict.map(tokenize_function, batch_size=64, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b37eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_train = tokenized_datasets['train'].shuffle(seed=1234).select(range(5000))\n",
    "# shuffled_train = tokenized_datasets['train'].shuffle(seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610e555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(shuffled_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afed11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9f1b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1d0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c42d26",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir='./9.bert_tokenizer',\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  learning_rate=1e-5,\n",
    "                                  logging_steps=150,\n",
    "                                  per_device_train_batch_size=16,\n",
    "                                  num_train_epochs=10)\n",
    "\n",
    "trainer = Trainer(model=model,\n",
    "                  args=training_args,\n",
    "                  train_dataset=shuffled_train,\n",
    "                  eval_dataset=tokenized_datasets['test'],\n",
    "                  compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd71ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
